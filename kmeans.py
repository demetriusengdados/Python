# -*- coding: utf-8 -*-
"""kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sq71mqTS8Di3KWy1GOwmwJd3evSaoH5b
every next line is a new cell in jupyter notebook
"""

import os
os.environ["HADOOP_HOME"] = "/home/hadoop/hadoop2"
os.environ["SPARK_HOME"] = "/home/bhaven/spark-2.3.1-bin-hadoop2.7"
os.environ["JAVA_HOME"]="/usr/lib/jvm/java-1.8.0-openjdk-amd64"

!echo $HADOOP_HOME
!echo $SPARK_HOME
!echo $JAVA_HOME

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
spark

from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

# Loads data.
dataset = spark.read.format("libsvm").load("file:///home/bhaven/Downloads/sample_kmeans_data.txt")

# Trains a k-means model.
kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(dataset)

# Make predictions
predictions = model.transform(dataset)

# Evaluate clustering by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

# Shows the result.
centers = model.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)

dataset.show()

import pandas as pd
df=dataset.toPandas()
l = []
for i in range (0,len(df["features"])):
    l.append(df["features"][i][0])
df

import matplotlib.pyplot as plt
plt.scatter(l,df['label'],c='red')
plt.scatter(centers,centers,c="black",s=100)
plt.grid()
plt.xlabel("Label")
plt.ylabel("Features")
plt.show()
